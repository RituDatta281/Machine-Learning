{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Binarizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "# data.drop('customerID',axis=1,inplace=True)\n",
    "# data= pd.DataFrame(data)\n",
    "# print(data)\n",
    "# data['TotalCharges'] = data[\"TotalCharges\"].replace(\" \",0)\n",
    "# data['TotalCharges']= pd.to_numeric(data['TotalCharges'])\n",
    "# data['TotalCharges'] = data[\"TotalCharges\"].replace(0,data['TotalCharges'].mean())\n",
    "# data['TotalCharges']\n",
    "# target = data[\"Churn\"]\n",
    "# print (\"\\nMissing values :  \", data.isnull().sum().values.sum())\n",
    "# print (\"\\nUnique values :  \\n\",data.nunique())\n",
    "\n",
    "# cat = data.nunique()[data.nunique() < 6].keys().tolist()\n",
    "# target_col = [\"Churn\"]\n",
    "# cat = [x for x in cat if x not in target_col]\n",
    "# num_cols   = [x for x in data.columns if x not in cat + target_col ]\n",
    "# bin_cols = data.nunique()[data.nunique() == 2].keys().tolist()\n",
    "# bin_cols = [x for x in bin_cols if x not in target_col]\n",
    "# multi_cols = [x for x in cat if x not in bin_cols]\n",
    "\n",
    "# #binary encoding\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# for i in bin_cols:\n",
    "#     data[i] = le.fit_transform(data[i])\n",
    "#     print(data[i])\n",
    "\n",
    "# #string -> encoding\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# for i in multi_cols:\n",
    "#     data[i] = le.fit_transform(data[i])\n",
    "#     print(data[i])\n",
    "    \n",
    "# #Binarization\n",
    "# for i in range(len(num_cols)):\n",
    "#     binarizer= Binarizer(data[num_cols[i]].median())\n",
    "#     data[num_cols[i]] = binarizer.fit_transform(data[num_cols[i]].values.reshape(1,-1)).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['toothed', 'breathes', 'legs'], dtype='object')\n",
      "species\n"
     ]
    }
   ],
   "source": [
    " data = pd.DataFrame({\"toothed\":[\"True\",\"True\",\"True\",\"False\",\"True\",\"True\",\"True\",\"True\",\"True\",\"False\"],\"breathes\":[\"True\",\"True\",\"True\",\"True\",\"True\",\"True\",\"False\",\"True\",\"True\",\"True\"],\"legs\":[\"True\",\"True\",\"False\",\"True\",\"True\",\"True\",\"False\",\"False\",\"True\",\"True\"],\"species\":[\"Mammal\",\"Mammal\",\"Reptile\",\"Mammal\",\"Mammal\",\"Mammal\",\"Reptile\",\"Reptile\",\"Mammal\",\"Reptile\"]})\n",
    "# data = pd.read_csv(\"tennis.csv\")\n",
    "# data = pd.read_csv(\"hd.csv\")\n",
    "# print(data)\n",
    "features= data.columns[:-1]\n",
    "print(features)\n",
    "target = data.columns[features.size]\n",
    "print(target)\n",
    "columns=data.columns\n",
    "# data\n",
    "\n",
    "# y=data[target]\n",
    "# data.drop(target,axis=1,inplace=True)\n",
    "# x=np.array(data)\n",
    "# x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.2)\n",
    "# print(x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(column):\n",
    "    elements,counts = np.unique(column,return_counts = True)\n",
    "    entropy = 0\n",
    "    for i in range(elements.size):\n",
    "        entropy += counts[i]/np.sum(counts)*log(counts[i]/np.sum(counts),2)\n",
    "    entropy= -entropy\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InfoGain(data,attribute_name):\n",
    "    attribute= data.get(attribute_name)\n",
    "    total_entropy = entropy(data.get(target))\n",
    "    elements,counts = np.unique(attribute,return_counts = True)\n",
    "    split_columns={}\n",
    "    for i in range(elements.size):\n",
    "        split_columns[i] = data[attribute == elements[i]]  \n",
    "    weighted_Entropy = 0\n",
    "    for i in range(elements.size):\n",
    "        weighted_Entropy += (counts[i]/np.sum(counts))*entropy(split_columns[i].get(target))\n",
    "    Info_gain = total_entropy - weighted_Entropy \n",
    "    return Info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PLURALITY_VALUE(example):\n",
    "    elements,count = np.unique(example[target],return_counts=True)\n",
    "    selected_index =  np.argmax(count)\n",
    "#     print(np.argmax(count))\n",
    "#     print(np.unique(example[target])[selected_index])\n",
    "    return np.unique(example[target])[selected_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_learning(examples, attributes, parent_examples, depth, tree):\n",
    "    \n",
    "# if examples is empty then return PLURALITY_VALUE(parent_examples)\n",
    "    if depth == 0:\n",
    "        tree = PLURALITY_VALUE(examples)\n",
    "        return tree\n",
    "    \n",
    "    if examples.size==0 :\n",
    "        return PLURALITY_VALUE(parent_examples)\n",
    "    \n",
    "# else if all examples have same classification then return the classification\n",
    "    elif np.unique(examples[target]).size < 2:\n",
    "        return np.unique(examples[target])[0]\n",
    "    \n",
    "# else if attributes is empty then return PLURALITY_VALUE(examples)\n",
    "    elif attributes.size == 0:\n",
    "        return PLURALITY_VALUE(examples)\n",
    "    \n",
    "    else:\n",
    "        parent_examples = PLURALITY_VALUE(examples)\n",
    "        \n",
    "        InfoGain_values = np.empty((0, attributes.size))\n",
    "        for feature in attributes:\n",
    "            InfoGain_values = np.append(InfoGain_values,InfoGain(examples,feature))\n",
    "        \n",
    "        selected_column = examples.get(attributes[np.argmax(InfoGain_values)])\n",
    "        best_attribute = attributes[np.argmax(InfoGain_values)]\n",
    "        \n",
    "        tree = {best_attribute:{}}\n",
    "       \n",
    "        featuresNew = np.empty((0, attributes.size-1))\n",
    "        for i in range(attributes.size):\n",
    "            if(attributes[i]!=best_attribute):\n",
    "                featuresNew = np.append(featuresNew,attributes[i])\n",
    "                \n",
    "        elements,counts=np.unique(selected_column,return_counts=True)\n",
    "        \n",
    "        for i in range((np.unique(selected_column)).size):\n",
    "            value = np.unique(selected_column)[i]\n",
    "            exs = examples.get(examples[best_attribute] == value)\n",
    "            subtree = decision_tree_learning(exs,featuresNew,parent_examples, depth-1, tree)\n",
    "            tree[best_attribute][value] = subtree\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'legs': {'False': 'Reptile', 'True': 'Mammal'}}\n"
     ]
    }
   ],
   "source": [
    "tree={}\n",
    "tree= decision_tree_learning(data,features, None,1, tree )\n",
    "pprint(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize(vector):\n",
    "    return vector / np.linalg.norm(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaBoost(examples,L,K):\n",
    "#     examples, set of N labeled examples (x , y ), ... , (x , y )\n",
    "#     L, a learning algorithm\n",
    "#     K, the number of hypotheses in the ensemble\n",
    "    w = np.full((1,len(examples)), 1/len(examples)) #w, a vector of N example weights, initially 1 N\n",
    "    h = np.full((1,K), 0) #h, a vector of K hypotheses\n",
    "    z = np.full((1,K), 0) #z, a vector of K hypothesis weights\n",
    "    \n",
    "    for k in K:\n",
    "        data = Resample(examples, w)\n",
    "        h[k] = L(data)\n",
    "        error = 0\n",
    "        for j in len(examples):\n",
    "            if h[k].get(xj)!=yj :\n",
    "                error = error + w[j]\n",
    "        if error > 0.5 :\n",
    "            continue\n",
    "        for j in len(examples):\n",
    "            if h[k](xj) = yj:\n",
    "                w[j] = w[j] * error/(1 âˆ’ error)\n",
    "        w = Normalize(w)\n",
    "        z[k] = log ((1-error)/error)\n",
    "        \n",
    "    return Weighted_Majority(h,z)             \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 4) (6,) (2, 4) (2,)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
