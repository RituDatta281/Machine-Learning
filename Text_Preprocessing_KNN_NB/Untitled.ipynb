{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from copy import copy, deepcopy\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Coffee', 'Arduino', 'Anime']\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "with open('Data/topics.txt','r',encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line[:-1]\n",
    "        inputs.append(line)\n",
    "    print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Text_Preprocessing(input_type):\n",
    "    input_file = \"Data/Training/\"+ input_type + \".xml\"\n",
    "    array = []\n",
    "    with open(input_file,'r',encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        soup = bs(content)\n",
    "        for items in soup.findAll(\"row\"):\n",
    "            Document = dict()\n",
    "            body = items['body']\n",
    "\n",
    "            # removing tags and numbers using regex        \n",
    "#             body = re.sub(r'<[^>]*>','', body)\n",
    "            body = re.sub(r'[-+]?\\d+', '', body)\n",
    "            body = re.sub(r'[^\\x00-\\x7F]', ' ', body)\n",
    "\n",
    "            #Lowercase the text\n",
    "            body = body.lower()\n",
    "\n",
    "            #Remove punctuations\n",
    "            body = body.translate((str.maketrans('','',string.punctuation)))\n",
    "\n",
    "            #Tokenize\n",
    "            body = word_tokenize(body)\n",
    "\n",
    "            #Remove stopwords\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            body = [word for word in body if not word in stop_words]\n",
    "\n",
    "            #Lemmatize tokens\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            body = [lemmatizer.lemmatize(word) for word in body]\n",
    "\n",
    "            #Stemming tokens\n",
    "            stemmer= PorterStemmer()\n",
    "            body = [stemmer.stem(word) for word in body]\n",
    "            \n",
    "            if not body:\n",
    "                continue\n",
    "            \n",
    "            Document[input_type] = body\n",
    "            \n",
    "            array.append(Document)\n",
    "            #print(body, \"\\n\\n\\n\")\n",
    "        #print(array)\n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_set = []\n",
    "Validation_set = []\n",
    "Test_set = []\n",
    "for input_file in inputs:\n",
    "    array = Text_Preprocessing(input_file)\n",
    "    Training_set += array[:500]\n",
    "    Validation_set += array[500:500+200]\n",
    "    Test_set += array[700:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "600\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "print(len(Training_set))\n",
    "print(len(Validation_set))\n",
    "print(len(Test_set))\n",
    "# print(Training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17306\n"
     ]
    }
   ],
   "source": [
    "FeatureSpace = []\n",
    "for document in Training_set:\n",
    "    temp = list(document.values())\n",
    "    for i in temp[0]:\n",
    "        if i not in FeatureSpace:\n",
    "            FeatureSpace.append(i)\n",
    "print(len(FeatureSpace))\n",
    "# print(FeatureSpace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = []\n",
    "X_train_HD = []\n",
    "X_train_ED = []\n",
    "\n",
    "X_Validation_HD = []\n",
    "X_Validation_ED = []\n",
    "Y_Validation =[]\n",
    "\n",
    "X_test_HD = []\n",
    "X_test_ED = []\n",
    "Y_test = []\n",
    "\n",
    "for i in range(len(Training_set)):\n",
    "    Y_train.append(list(Training_set[i].keys())[0])\n",
    "    temp = list(Training_set[i].values())[0]\n",
    "    HD = []\n",
    "    ED = []\n",
    "    for i in range(len(FeatureSpace)):\n",
    "        if FeatureSpace[i] in temp:\n",
    "            HD.append(1)\n",
    "        else:\n",
    "            HD.append(0)\n",
    "        \n",
    "        ED.append(temp.count(FeatureSpace[i]))\n",
    "        \n",
    "    X_train_HD.append(HD)\n",
    "    X_train_ED.append(ED)\n",
    "\n",
    "for i in range(len(Test_set)):\n",
    "    Y_test.append(list(Test_set[i].keys())[0])\n",
    "    temp = list(Test_set[i].values())[0]\n",
    "    HD = []\n",
    "    ED = []\n",
    "    for i in range(len(FeatureSpace)):\n",
    "        if FeatureSpace[i] in temp:\n",
    "            HD.append(1)\n",
    "        else:\n",
    "            HD.append(0)\n",
    "        \n",
    "        ED.append(temp.count(FeatureSpace[i]))\n",
    "        \n",
    "    X_test_HD.append(HD)\n",
    "    X_test_ED.append(ED)\n",
    "    \n",
    "    \n",
    "for i in range(len(Validation_set)):\n",
    "    Y_Validation.append(list(Validation_set[i].keys())[0])\n",
    "    temp = list(Validation_set[i].values())[0]\n",
    "    HD = []\n",
    "    ED = []\n",
    "    for i in range(len(FeatureSpace)):\n",
    "        if FeatureSpace[i] in temp:\n",
    "            HD.append(1)\n",
    "        else:\n",
    "            HD.append(0)\n",
    "        \n",
    "        ED.append(temp.count(FeatureSpace[i]))\n",
    "        \n",
    "    X_Validation_HD.append(HD)\n",
    "    X_Validation_ED.append(ED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17306\n"
     ]
    }
   ],
   "source": [
    "print(len(X_Validation_HD[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDistance(Dt,D1):\n",
    "    count = 0\n",
    "    for i in range(len(D1)):\n",
    "        if D1[i] != Dt[i]:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HammingDistance(Dt, X_train, Y_train, k = 3):\n",
    "    Hamming_D = []\n",
    "    for i in range(len(X_train)):\n",
    "        Hamming_D.append(getDistance(Dt, X_train[i]))\n",
    "        \n",
    "#     print(Hamming_D)\n",
    "    \n",
    "    indices = sorted(range(len(Hamming_D)), key = lambda sub: Hamming_D[sub])[:k] \n",
    "    Y_output = []\n",
    "    for i in indices:\n",
    "        Y_output.append(Y_train[i])   \n",
    "    output = max(set(Y_output), key = Y_output.count) \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HammingDistance_Accuracy(X_Validation_HD, Y_Validation, X_train_HD, Y_train):\n",
    "    count = 0\n",
    "    for i in range(len(X_Validation_HD)):\n",
    "        if Y_Validation[i] == HammingDistance(X_Validation_HD[i], X_train_HD, Y_train):\n",
    "            count += 1\n",
    "            print(count)\n",
    "    print(\"Accuracy is: \", (count/len(Y_Validation) * 100))\n",
    "\n",
    "# HammingDistance(X_Validation_HD[0], X_train_HD, Y_train)\n",
    "# print(Y_Validation[0])\n",
    "# HammingDistance_Accuracy(X_Validation_HD, Y_Validation, X_train_HD, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateED(Dt,D):\n",
    "    value = 0\n",
    "    for i in range(len(D)):\n",
    "        value += pow(Dt[i] - D[i], 2)\n",
    "    value = math.sqrt(value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EuclideanDistance(Dt, X_train, Y_train, k):\n",
    "    Euclidean_D = []\n",
    "    for i in range(len(X_train)):\n",
    "        Euclidean_D.append(CalculateED(Dt, X_train[i]))\n",
    "    \n",
    "    #find k minimum indices\n",
    "    indices = sorted(range(len(Euclidean_D)), key = lambda sub: Euclidean_D[sub])[:k] \n",
    "    Y_output = []\n",
    "    for i in indices:\n",
    "        Y_output.append(Y_train[i])   \n",
    "    \n",
    "    #find higher frequency result\n",
    "    output = max(set(Y_output), key = Y_output.count) \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EuclideanDistance_Accuracy(X_Validation_ED, Y_Validation, X_train_ED, Y_train, k):\n",
    "    count = 0\n",
    "    for i in range(len(X_Validation_ED)):\n",
    "        if Y_Validation[i] == EuclideanDistance(X_Validation_ED[i], X_train_ED, Y_train, k):\n",
    "            count += 1\n",
    "    print(\"Accuracy is: \", (count/len(Y_Validation) * 100))\n",
    "\n",
    "# HammingDistance(X_Validation_HD[0], X_train_HD, Y_train)\n",
    "# print(Y_Validation[0])\n",
    "EuclideanDistance_Accuracy(X_Validation_ED, Y_Validation, X_train_ED, Y_train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anime\n",
      "Anime\n"
     ]
    }
   ],
   "source": [
    "EuclideanDistance(X_test_ED[1400], X_train_ED.copy(), Y_train.copy())\n",
    "print(Y_test[1400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_documents = len(X_train_ED_COPY)\n",
    "d_word = np.sum(X_train_HD.copy(), axis=0)\n",
    "# print(sum(d_word))\n",
    "# print(len(d_word))\n",
    "# print(number_of_documents)\n",
    "IDF = []\n",
    "for i in range(len(d_word)):\n",
    "    temp = math.log2(number_of_documents/d_word[i])\n",
    "    if temp <= 0:\n",
    "        IDF.append(0.0001)\n",
    "    else:\n",
    "        IDF.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setWeight(D, Dt):\n",
    "    totalWords_D = sum(D)\n",
    "    totalWords_Dt = sum(Dt)\n",
    "    for i in range(len(D)):\n",
    "        TF_D = D[i]/totalWords_D\n",
    "        TF_Dt = Dt[i]/totalWords_Dt\n",
    "        if D[i] != 0:\n",
    "            D[i] = TF_D * IDF[i]\n",
    "        if Dt[i] != 0 :\n",
    "            Dt[i] = TF_Dt * IDF[i]\n",
    "    return D,Dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CosineTheta(D , Dt):\n",
    "    D, Dt = setWeight (D,Dt)\n",
    "    dotProduct = np.dot(D,Dt)\n",
    "    length_D = math.sqrt(np.dot(D, D))\n",
    "    length_Dt = math.sqrt(np.dot(Dt, Dt))\n",
    "    return dotProduct/(length_D*length_Dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CosineSimilarity(Dt, X_train, Y_train, k):\n",
    "    Cosine_values = []\n",
    "    for D in X_train:\n",
    "        Cosine_values.append(CosineTheta(D,Dt))\n",
    "    indices = sorted(range(len(Cosine_values)), key = lambda sub: Cosine_values[sub])[-k:]\n",
    "    Y_output = []\n",
    "    for i in indices:\n",
    "        Y_output.append(Y_train[i])\n",
    "        #find higher frequency result\n",
    "    output = max(set(Y_output), key = Y_output.count)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anime\n",
      "Anime\n"
     ]
    }
   ],
   "source": [
    "CosineSimilarity(X_test_ED[1400].copy(), deepcopy(X_train_ED), deepcopy(Y_train), 3)\n",
    "print(Y_test[1400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denominatorOfNB(Dt):\n",
    "    summation = 0\n",
    "    for i in range(len(TopicName)):\n",
    "        summation += Prob_DT_CM(Dt,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prob_Wj_Cm(word,m):\n",
    "    frequencyOf_W_in_M = 0\n",
    "    TotalWords_in_M = 0\n",
    "    for i in range(len(CM_list[m])):\n",
    "        frequencyOf_W_in_M += CM_list[m][i].count(word)\n",
    "        TotalWords_in_M += len(CM_list[m][i])\n",
    "    return (frequencyOf_W_in_M/TotalWords_in_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prob_DT_CM(Dt, m):\n",
    "    P_Dt_Cm = 1\n",
    "    for i in range(len(Dt)):\n",
    "        P_Dt_Cm *= Prob_Wj_Cm(FeatureSpace[i],m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(m):\n",
    "    Cm_count = 0\n",
    "    for i in range(len(CM_list[m])):\n",
    "        Cm_count += len(CM_list[m][i])\n",
    "    return Cm_count/TotalWords_in_TrainingSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaiveBayes(Dt, m):\n",
    "    denom = denominatorOfNB(Dt)\n",
    "    value = (Prob_DT_CM(Dt,m) * P(Cm))/denom\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicName = []\n",
    "x=[d.keys() for d in Training_set]\n",
    "for keys in x:\n",
    "    if list(keys) not in TopicName:\n",
    "        TopicName.append(list(keys))\n",
    "CM_list = []\n",
    "for j in range(len(TopicName)):\n",
    "    CM_list.append([])\n",
    "    for i in range(len(Training_set)):\n",
    "        if list(Training_set[i].keys()) == TopicName[0]:\n",
    "            y = list(Training_set[i].values())[0]\n",
    "            CM_list[j].append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# for i in range(len(CM_list[0])):\n",
    "print(len(CM_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "TotalWords_in_TrainingSet = 0\n",
    "for i in range(len(CM_list)):\n",
    "    for j in range(len(CM_list[i])):\n",
    "        TotalWords_in_TrainingSet += len(CM_list[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118560\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
