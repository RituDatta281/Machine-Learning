{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import math\n",
    "import random\n",
    "from copy import copy, deepcopy\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from numpy.linalg import norm\n",
    "import time\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputFiles():\n",
    "    inputs = []\n",
    "    with open('Data/topics.txt','r',encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line[:-1]\n",
    "            inputs.append(line)\n",
    "        print(inputs)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Text_Preprocessing(input_type):\n",
    "    input_file = \"Data/Training/\"+ input_type + \".xml\"\n",
    "    array = np.zeros(0)\n",
    "    with open(input_file,'r',encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        soup = bs(content,'lxml')\n",
    "        for items in soup.findAll(\"row\"):\n",
    "            Document = dict()\n",
    "            body = items['body']\n",
    "               \n",
    "            #removing links\n",
    "            body = re.sub(r\"<a.*</a>\",\"\", body)\n",
    "            \n",
    "            #removing tags\n",
    "            body = re.sub(\"<.[^>]*>\",\"\", body)\n",
    "            \n",
    "            #removing unicode\n",
    "            body = re.sub(r'[^\\x00-\\x7F]', ' ', body)\n",
    "            \n",
    "            #removing numbers\n",
    "            body = re.sub(r'[-+]?\\d+', '', body)\n",
    "\n",
    "            #Lowercase the text\n",
    "            body = body.lower()\n",
    "\n",
    "            #Remove punctuations\n",
    "            body = body.translate((str.maketrans('','',string.punctuation)))\n",
    "\n",
    "            #Tokenize\n",
    "            body = word_tokenize(body)\n",
    "\n",
    "            #Remove stopwords\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            body = [word for word in body if not word in stop_words]\n",
    "\n",
    "            #Lemmatize tokens\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            body = [lemmatizer.lemmatize(word) for word in body]\n",
    "\n",
    "            #Stemming tokens\n",
    "            stemmer= PorterStemmer()\n",
    "            body = [stemmer.stem(word) for word in body]\n",
    "            \n",
    "            if not body:\n",
    "                continue\n",
    "            \n",
    "            Document[input_type] = body\n",
    "            \n",
    "            array = np.append(array,Document)\n",
    "            #print(body, \"\\n\\n\\n\")\n",
    "        #print(array)\n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_dataSets(inputs):\n",
    "    Training_set = np.zeros(0)\n",
    "    Validation_set = np.zeros(0)\n",
    "    Test_set = np.zeros(0)\n",
    "    for input_file in inputs:\n",
    "        print(input_file)\n",
    "        array = Text_Preprocessing(input_file)\n",
    "        Training_set = np.append(Training_set, array[:500])\n",
    "        Validation_set = np.append(Validation_set,array[500:500+200])\n",
    "        Test_set = np.append(Test_set,array[700:1200])\n",
    "    return Training_set, Validation_set, Test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5500\n",
      "2200\n",
      "5500\n"
     ]
    }
   ],
   "source": [
    "print(len(Training_set))\n",
    "print(len(Validation_set))\n",
    "print(len(Test_set))\n",
    "# print(Training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureSpace_Set(Training_set):\n",
    "    FeatureSpace = []\n",
    "    for document in Training_set:\n",
    "        temp = list(document.values())\n",
    "        for i in temp[0]:\n",
    "            if i not in FeatureSpace:\n",
    "                FeatureSpace.append(i)\n",
    "    print(len(FeatureSpace))\n",
    "    return FeatureSpace\n",
    "# print(FeatureSpace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def documentsSet (Training_set,Validation_set, FeatureSpace):\n",
    "    \n",
    "    Y_train = []\n",
    "    X_train_HD = []\n",
    "    X_train_ED = []\n",
    "\n",
    "    X_Validation_HD = []\n",
    "    X_Validation_ED = []\n",
    "    Y_Validation =[]\n",
    "\n",
    "    for i in range(len(Training_set)):\n",
    "        Y_train.append(list(Training_set[i].keys())[0])\n",
    "        temp = list(Training_set[i].values())[0]\n",
    "        HD = []\n",
    "        ED = []\n",
    "        for i in range(len(FeatureSpace)):\n",
    "            if FeatureSpace[i] in temp:\n",
    "                HD.append(1)\n",
    "            else:\n",
    "                HD.append(0)\n",
    "\n",
    "            ED.append(temp.count(FeatureSpace[i]))\n",
    "\n",
    "        X_train_HD.append(HD)\n",
    "        X_train_ED.append(ED)\n",
    "\n",
    "    print(\"TRAIN DONE\")    \n",
    "\n",
    "    for i in range(len(Validation_set)):\n",
    "        Y_Validation.append(list(Validation_set[i].keys())[0])\n",
    "        temp = list(Validation_set[i].values())[0]\n",
    "        HD = []\n",
    "        ED = []\n",
    "        for i in range(len(FeatureSpace)):\n",
    "            if FeatureSpace[i] in temp:\n",
    "                HD.append(1)\n",
    "            else:\n",
    "                HD.append(0)\n",
    "\n",
    "            ED.append(temp.count(FeatureSpace[i]))\n",
    "\n",
    "        X_Validation_HD.append(HD)\n",
    "        X_Validation_ED.append(ED)\n",
    "        \n",
    "    return X_train_ED, X_train_HD, Y_train, X_Validation_ED, X_Validation_HD,Y_Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDistance(Dt,D1):\n",
    "    count = 0\n",
    "    for i in range(len(D1)):\n",
    "        if D1[i] != Dt[i]:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HammingDistance(Dt, X_train, Y_train):\n",
    "    Hamming_D = []\n",
    "    for i in range(len(X_train)):\n",
    "        Hamming_D.append(len(Dt)*distance.hamming(Dt, X_train[i]))\n",
    "#         Hamming_D.append(getDistance(Dt, X_train[i]))\n",
    "    \n",
    "    output = []\n",
    "#     print(Hamming_D)\n",
    "    for k in K:\n",
    "        indices = sorted(range(len(Hamming_D)), key = lambda sub: Hamming_D[sub])[:k] \n",
    "        Y_output = []\n",
    "        for i in indices:\n",
    "            Y_output.append(Y_train[i])   \n",
    "    \n",
    "        output.append(max(set(Y_output), key = Y_output.count))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def HammingDistance_Accuracy(X_Validation_HD, Y_Validation, X_train_HD, Y_train):\n",
    "    count = [0,0,0]\n",
    "    for i in range(len(X_Validation_HD)):\n",
    "        result = HammingDistance(X_Validation_HD[i], X_train_HD, Y_train)\n",
    "        \n",
    "        for k in range(len(K)):\n",
    "            if Y_Validation[i] == result[k] :\n",
    "                count[k] += 1\n",
    "        \n",
    "        print(i, count)\n",
    "        \n",
    "    for k in range(len(K)):\n",
    "        print(\"Accuracy is: \", (count[k]/len(Y_Validation) * 100))\n",
    "\n",
    "# print(HammingDistance(X_test_HD[1200], X_train_HD, Y_train))\n",
    "# print(Y_test[1200])\n",
    "# HammingDistance_Accuracy(np.array(X_Validation_HD), np.array(Y_Validation), np.array(X_train_HD), np.array(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EuclideanDistance(Dt, X_train, Y_train):\n",
    "    Euclidean_D = []\n",
    "    for i in range(len(X_train)):\n",
    "#         Euclidean_D.append(CalculateED(Dt, X_train[i]))\n",
    "        Euclidean_D.append(distance.euclidean(Dt, X_train[i]))\n",
    "        \n",
    "    \n",
    "    output = []\n",
    "    for k in K:\n",
    "    #find k minimum indices\n",
    "        indices = sorted(range(len(Euclidean_D)), key = lambda sub: Euclidean_D[sub])[:k] \n",
    "        Y_output = []\n",
    "        for i in indices:\n",
    "            Y_output.append(Y_train[i])   \n",
    "    \n",
    "        #find higher frequency result\n",
    "        output.append(max(set(Y_output), key = Y_output.count)) \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EuclideanDistance_Accuracy(X_Validation_ED, Y_Validation, X_train_ED, Y_train):\n",
    "    count = [0, 0, 0]\n",
    "    for i in range(len(X_Validation_ED)):\n",
    "        result = EuclideanDistance(X_Validation_ED[i], X_train_ED, Y_train)\n",
    "        \n",
    "        for k in range(len(K)):\n",
    "            if Y_Validation[i] == result[k] :\n",
    "                count[k] += 1\n",
    "        \n",
    "        print(i,count)\n",
    "    \n",
    "    for k in range(len(K)):\n",
    "        print(\"Accuracy is: \", (count[k]/len(Y_Validation) * 100))\n",
    "\n",
    "# HammingDistance(X_Validation_HD[0], X_train_HD, Y_train)\n",
    "# print(Y_Validation[0])\n",
    "\n",
    "# EuclideanDistance_Accuracy(np.array(X_Validation_ED), np.array(Y_Validation), np.array(X_train_ED), np.array(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDF_SET(X_train_HD):\n",
    "    number_of_documents = len(X_train_HD)\n",
    "    d_word = np.sum(X_train_HD.copy(), axis=0)\n",
    "\n",
    "    IDF = []\n",
    "    for i in range(len(d_word)):\n",
    "        temp = math.log2(number_of_documents/d_word[i])\n",
    "        if temp <= 0:\n",
    "            IDF.append(0.0001)\n",
    "        else:\n",
    "            IDF.append(temp)\n",
    "    return IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setWeightOfDocument(D, IDF):\n",
    "    totalWords_D = sum(D)\n",
    "    for i in range(len(D)):\n",
    "        if D[i] != 0 :\n",
    "            D[i] = (D[i]/totalWords_D) * IDF[i]\n",
    "    return D\n",
    "\n",
    "def SetAllWeightOfSET(X_train, IDF):\n",
    "#     count = 0\n",
    "    for D in X_train:\n",
    "        D = setWeightOfDocument(D, IDF)\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = SetAllWeightOfSET(deepcopy(X_train_ED))\n",
    "V = SetAllWeightOfSET(deepcopy(X_Validation_ED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CosineTheta(D , Dt):\n",
    "    return np.dot(D,Dt)/(norm(D)*norm(Dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CosineSimilarity(Dt, X_train, Y_train):\n",
    "    Cosine_values = []\n",
    "    output = []\n",
    "    for D in X_train:\n",
    "        Cosine_values.append(1-distance.cosine(Dt,D))\n",
    "\n",
    "    for k in K:\n",
    "        indices = (sorted(range(len(Cosine_values)), key = lambda sub: Cosine_values[sub])[-k:])\n",
    "        Y_output = []\n",
    "        for i in indices:\n",
    "            Y_output.append(Y_train[i])\n",
    "    \n",
    "        output.append(max(set(Y_output), key = Y_output.count)) \n",
    "#     print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def CosineValidation_Test(X_Validation_ED, Y_Validation, X_train_ED, Y_train)    \n",
    "    count = [0,0,0]\n",
    "#     count = 0\n",
    "    for i in range(len(X_Validation_ED)):\n",
    "        result = CosineSimilarity(X_Validation_ED[i], X_train_ED, Y_train)\n",
    "        \n",
    "        for k in range(len(K)):\n",
    "            if Y_Validation[i] == result[k]:\n",
    "                count[k] += 1\n",
    "#         if Y_Validation[i] ==  CosineSimilarity(X_Validation_ED[i], X_train_ED, Y_train):\n",
    "#             count+= 1\n",
    "        \n",
    "        print(i, count)\n",
    "    \n",
    "    for k in range(len(K)):\n",
    "        print(\"Accuracy is: \", (count[k]/len(Y_Validation) * 100))\n",
    "    \n",
    "#     return (count/len(Y_Validation) * 100)\n",
    "\n",
    "CosineValidation_Test(np.array(V), deepcopy(np.array(Y_Validation)), np.array(X), deepcopy(np.array(Y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denominatorOfNB(Dt,alpha):\n",
    "    summation = 0\n",
    "    \n",
    "    for i in range(len(TopicName)):\n",
    "        summation += Prob_DT_CM(Dt,i, alpha)\n",
    "    \n",
    "    return summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prob_Wj_Cm(word, m, alpha):\n",
    "    \n",
    "    frequencyOf_W_in_M = 0\n",
    "    TotalWords_in_M = len(CM_list[m])\n",
    "#     print(TotalWords_in_M)\n",
    "#     print(len(FeatureSpace))\n",
    "    \n",
    "    for i in range(len(CM_list[m])):\n",
    "        frequencyOf_W_in_M += CM_list[m][i].count(word)\n",
    "        \n",
    "    value = (frequencyOf_W_in_M + alpha)/ (TotalWords_in_M + alpha * len(FeatureSpace))\n",
    "    \n",
    "    if value == 0:\n",
    "        print(m, \"e jhamela ache \", value, \"->\", word)\n",
    "\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prob_DT_CM(Dt, m, alpha):\n",
    "    P_Dt_Cm = 1.0\n",
    "    \n",
    "    for i in range(len(Dt)):\n",
    "        if(Dt[i] != 0):\n",
    "            P_Dt_Cm *= Prob_Wj_Cm(FeatureSpace[i],m, alpha)\n",
    "\n",
    "    return P_Dt_Cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaiveBayes(Dt, m, denom, alpha):\n",
    "    value = (Prob_DT_CM(Dt,m,alpha) * (1/len(CM_list)))/ (denom + alpha*len(FeatureSpace))\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB_precision(X_Validation_HD, Y_Validation, alpha):\n",
    "    count = 0\n",
    "    for itr in range(len(X_Validation_HD)):\n",
    "        Probability = []\n",
    "        denom = denominatorOfNB(X_Validation_HD[itr], alpha)\n",
    "#         if denom == 0:\n",
    "#             continue\n",
    "        for i in range(len(CM_list)):\n",
    "            Probability.append(NaiveBayes(X_Validation_HD[itr],i, denom, alpha))\n",
    "        \n",
    "        prediction = TopicName[Probability.index(max(Probability))][0]\n",
    "        if prediction == Y_Validation[itr]:\n",
    "            count += 1\n",
    "        print(count)\n",
    "\n",
    "    return ((count/len(Y_Validation)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestValue(itr, Test_set, FeatureSpace):\n",
    "#     print(len(Test_set))\n",
    "    X_test_HD = []\n",
    "    X_test_ED = []\n",
    "    Y_test = []\n",
    "\n",
    "    # print(inputs)\n",
    "    Test_Final = []\n",
    "    for input_ in inputs:\n",
    "#         print(input_)\n",
    "        temp = [Test for Test in Test_set if list(Test.keys())[0] == input_ ]\n",
    "        Test_Final += temp[itr*10:itr*10+10]\n",
    "\n",
    "    for i in range(len(Test_Final)):\n",
    "        Y_test.append(list(Test_Final[i].keys())[0])\n",
    "        temp = list(Test_Final[i].values())[0]\n",
    "        HD = []\n",
    "        ED = []\n",
    "        \n",
    "        for i in range(len(FeatureSpace)):\n",
    "            if FeatureSpace[i] in temp:\n",
    "                HD.append(1)\n",
    "            else:\n",
    "                HD.append(0)\n",
    "                \n",
    "            ED.append(temp.count(FeatureSpace[i]))\n",
    "                \n",
    "        X_test_HD.append(HD)\n",
    "        X_test_ED.append(ED)\n",
    "        \n",
    "    return X_test_HD, Y_test, X_test_ED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_NB_CS(Test_set,FeatureSpace,X_test_HD,Y_test, X_test_ED, X, Y_train):\n",
    "    for iteration in range(50):\n",
    "        X_test_HD, Y_test, X_test_ED = getTestValue(iteration, Test_set, FeatureSpace)\n",
    "        print(\"TestData extracted\")\n",
    "\n",
    "        accuracy_NB = NB_precision(X_test_HD, Y_test, 0.005)\n",
    "        print(\"Done iteration NB\", iteration, accuracy_NB)\n",
    "        with open('NaiveBayes.txt', 'a') as f:\n",
    "            f.write(\"%s\\n\" %accuracy_NB)\n",
    "\n",
    "        X_TEST = SetAllWeightOfSET(deepcopy(X_test_ED))\n",
    "        print(\"X_Test weight set\")\n",
    "        accuracy_CS = CosineValidation_Test(X_TEST, deepcopy(Y_test), X, deepcopy(Y_train))\n",
    "        print(\"Done iteration CS\", iteration, accuracy_CS)    \n",
    "        with open('CosineSim.txt', 'a') as f2:\n",
    "            f2.write(\"%s\\n\" %accuracy_CS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_TSTAT():\n",
    "    NB = []\n",
    "    with open('NaiveBayes.txt', 'r') as f:\n",
    "        filecontents = f.readlines()\n",
    "        for line in filecontents:\n",
    "            current_place = line[:-1]\n",
    "            NB.append(float(current_place))\n",
    "\n",
    "    CS = []\n",
    "    with open('CosineSim.txt', 'r') as f2:\n",
    "        filecontents = f2.readlines()\n",
    "        for line in filecontents:\n",
    "            current_place = line[:-1]\n",
    "            CS.append(float(current_place))\n",
    "    \n",
    "    \n",
    "    print(\"Average value of Naive Bayes :\", sum(NB)/len(NB))\n",
    "    print(\"Average value of Cosine Simulation : \",sum(CS)/len(CS))\n",
    "    \n",
    "    significance_level = [0.005, 0.01, 0.05]\n",
    "    t_stat, p_val = stats.ttest_rel(NB,CS)\n",
    "    print(\"t-stat value = \",t_stat, \"and p_value =\", p_val)\n",
    "    for alpha in significance_level:\n",
    "        print(\"When alpha = \",alpha)\n",
    "        if p_val > alpha:\n",
    "            print('Accept null hypothesis that the means are equal.')\n",
    "        else:\n",
    "            print('Reject the null hypothesis that the means are equal.')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value of Naive Bayes : 83.20000000000002\n",
      "Average value of Cosine Simulation :  81.50909090909092\n",
      "t-stat value =  3.3134598364347445 and p_value = 0.0017378416481416729\n",
      "When alpha =  0.005\n",
      "Reject the null hypothesis that the means are equal.\n",
      "When alpha =  0.01\n",
      "Reject the null hypothesis that the means are equal.\n",
      "When alpha =  0.05\n",
      "Reject the null hypothesis that the means are equal.\n"
     ]
    }
   ],
   "source": [
    "# inputs = inputFiles()\n",
    "# Training_set, Validation_set, Test_set = set_dataSets(inputs)\n",
    "# FeatureSpace = featureSpace_Set(Training_set)\n",
    "# X_train_ED, X_train_HD, Y_train, X_Validation_ED, X_Validation_HD, Y_Validation = documentsSet(Training_set,Validation_set, FeatureSpace)\n",
    "# K = [1,3,5]\n",
    "# #for Hamming distance, k=1.3,5\n",
    "# HammingDistance_Accuracy(np.array(X_Validation_HD), np.array(Y_Validation), np.array(X_train_HD), np.array(Y_train))\n",
    "# #for Euclidean distance, k=1.3,5\n",
    "# EuclideanDistance_Accuracy(np.array(X_Validation_ED), np.array(Y_Validation), np.array(X_train_ED), np.array(Y_train))\n",
    "\n",
    "\n",
    "# # for cosine\n",
    "# IDF = IDF_SET(X_train_HD)\n",
    "# X = SetAllWeightOfSET(deepcopy(X_train_ED), IDF)\n",
    "# V = SetAllWeightOfSET(deepcopy(X_Validation_ED), IDF)\n",
    "\n",
    "# CosineValidation_Test(np.array(V), deepcopy(np.array(Y_Validation)), np.array(X), deepcopy(np.array(Y_train)))\n",
    "\n",
    "# Alpha_val = [1.5, 1, 0.5, 0.2, 0.1, 0.07, 0.05, 0.01, 0.005, 0.0001] \n",
    "\n",
    "# TopicName = []\n",
    "# x=[d.keys() for d in Training_set]\n",
    "# for keys in x:\n",
    "#     if list(keys) not in TopicName:\n",
    "#         TopicName.append(list(keys))\n",
    "\n",
    "# CM_list = []\n",
    "\n",
    "# for j in range(len(TopicName)):\n",
    "#     CM_list.append([])\n",
    "#     for i in range(len(Training_set)):\n",
    "#         if list(Training_set[i].keys()) == TopicName[j]:\n",
    "#             y = list(Training_set[i].values())[0]\n",
    "#             CM_list[j].append(y)\n",
    "\n",
    "# for alpha in Alpha_val:\n",
    "#     print(NB_precision(X_Validation_HD, Y_Validation, alpha))\n",
    "#     print(alpha)\n",
    "#     print(\"--------------------------\")\n",
    "\n",
    "# comparison_NB_CS(Test_set,FeatureSpace,X_test_HD,Y_test, X_test_ED, X, Y_train)\n",
    "\n",
    "calculate_TSTAT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
