{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import math\n",
    "import random\n",
    "from copy import copy, deepcopy\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from numpy.linalg import norm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Coffee', 'Arduino', 'Anime', 'Astronomy', 'Biology', 'Chess', 'Cooking', 'Law', 'Space', 'Windows_Phone', 'Wood_Working']\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "with open('Data/topics.txt','r',encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line[:-1]\n",
    "        inputs.append(line)\n",
    "    print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Text_Preprocessing(input_type):\n",
    "    input_file = \"Data/Training/\"+ input_type + \".xml\"\n",
    "    array = []\n",
    "    with open(input_file,'r',encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        soup = bs(content,'lxml')\n",
    "        for items in soup.findAll(\"row\"):\n",
    "            Document = dict()\n",
    "            body = items['body']\n",
    "               \n",
    "            #removing links\n",
    "            body = re.sub(r\"<a.*</a>\",\"\", body)\n",
    "            \n",
    "            #removing tags\n",
    "            body = re.sub(\"<.[^>]*>\",\"\", body)\n",
    "            \n",
    "            #removing unicode\n",
    "            body = re.sub(r'[^\\x00-\\x7F]', ' ', body)\n",
    "            \n",
    "            #removing numbers\n",
    "            body = re.sub(r'[-+]?\\d+', '', body)\n",
    "\n",
    "            #Lowercase the text\n",
    "            body = body.lower()\n",
    "\n",
    "            #Remove punctuations\n",
    "            body = body.translate((str.maketrans('','',string.punctuation)))\n",
    "\n",
    "            #Tokenize\n",
    "            body = word_tokenize(body)\n",
    "\n",
    "            #Remove stopwords\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            body = [word for word in body if not word in stop_words]\n",
    "\n",
    "            #Lemmatize tokens\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            body = [lemmatizer.lemmatize(word) for word in body]\n",
    "\n",
    "            #Stemming tokens\n",
    "            stemmer= PorterStemmer()\n",
    "            body = [stemmer.stem(word) for word in body]\n",
    "            \n",
    "            if not body:\n",
    "                continue\n",
    "            \n",
    "            Document[input_type] = body\n",
    "            \n",
    "            array.append(Document)\n",
    "            #print(body, \"\\n\\n\\n\")\n",
    "        #print(array)\n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_set = []\n",
    "# Validation_set = []\n",
    "Test_set = []\n",
    "for input_file in inputs:\n",
    "    array = Text_Preprocessing(input_file)\n",
    "    Training_set += array[:500]\n",
    "#     Validation_set += array[500:500+200]\n",
    "    Test_set += array[700:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5500\n",
      "2200\n",
      "5500\n"
     ]
    }
   ],
   "source": [
    "print(len(Training_set))\n",
    "print(len(Validation_set))\n",
    "print(len(Test_set))\n",
    "# print(Training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('TrainingSet.txt', 'w') as f:\n",
    "#     for item in Training_set:\n",
    "#         f.write(\"%s\\n\" % item)\n",
    "        \n",
    "        \n",
    "# with open('Validation_set.txt', 'w') as f:\n",
    "#     for item in Validation_set:\n",
    "#         f.write(\"%s\\n\" % item)\n",
    "\n",
    "FeatureSpace = []\n",
    "with open('FeatureSpace.txt', 'r') as f:\n",
    "    filecontents = f.readlines()\n",
    "    for line in f:\n",
    "        # remove linebreak which is the last character of the string\n",
    "        current_place = line[:-1]\n",
    "\n",
    "        # add item to the list\n",
    "        FeatureSpace.append(current_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18982\n"
     ]
    }
   ],
   "source": [
    "FeatureSpace = []\n",
    "for document in Training_set:\n",
    "    temp = list(document.values())\n",
    "    for i in temp[0]:\n",
    "        if i not in FeatureSpace:\n",
    "            FeatureSpace.append(i)\n",
    "print(len(FeatureSpace))\n",
    "# print(FeatureSpace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FeatureSpace.txt', 'w') as f:\n",
    "    for item in FeatureSpace:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Validation_set_S = random.choices(Validation_set, k = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = []\n",
    "X_train_HD = []\n",
    "X_train_ED = []\n",
    "\n",
    "# X_Validation_HD = []\n",
    "# X_Validation_ED = []\n",
    "# Y_Validation =[]\n",
    "\n",
    "# X_test_HD = []\n",
    "# X_test_ED = []\n",
    "# Y_test = []\n",
    "\n",
    "for i in range(len(Training_set)):\n",
    "    Y_train.append(list(Training_set[i].keys())[0])\n",
    "    temp = list(Training_set[i].values())[0]\n",
    "    HD = []\n",
    "    ED = []\n",
    "    for i in range(len(FeatureSpace)):\n",
    "        if FeatureSpace[i] in temp:\n",
    "            HD.append(1)\n",
    "        else:\n",
    "            HD.append(0)\n",
    "        \n",
    "        ED.append(temp.count(FeatureSpace[i]))\n",
    "        \n",
    "    X_train_HD.append(HD)\n",
    "    X_train_ED.append(ED)\n",
    "\n",
    "# for i in range(len(Test_set)):\n",
    "#     Y_test.append(list(Test_set[i].keys())[0])\n",
    "#     temp = list(Test_set[i].values())[0]\n",
    "#     HD = []\n",
    "#     ED = []\n",
    "#     for i in range(len(FeatureSpace)):\n",
    "#         if FeatureSpace[i] in temp:\n",
    "#             HD.append(1)\n",
    "#         else:\n",
    "#             HD.append(0)\n",
    "        \n",
    "#         ED.append(temp.count(FeatureSpace[i]))\n",
    "        \n",
    "#     X_test_HD.append(HD)\n",
    "#     X_test_ED.append(ED)\n",
    "    \n",
    "    \n",
    "# for i in range(len(Validation_set)):\n",
    "#     Y_Validation.append(list(Validation_set[i].keys())[0])\n",
    "#     temp = list(Validation_set[i].values())[0]\n",
    "#     HD = []\n",
    "#     ED = []\n",
    "#     for i in range(len(FeatureSpace)):\n",
    "#         if FeatureSpace[i] in temp:\n",
    "#             HD.append(1)\n",
    "#         else:\n",
    "#             HD.append(0)\n",
    "        \n",
    "#         ED.append(temp.count(FeatureSpace[i]))\n",
    "        \n",
    "#     X_Validation_HD.append(HD)\n",
    "#     X_Validation_ED.append(ED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = [1,3,5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_Validation_HD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-791af146aff3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_Validation_HD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(X_train_HD[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_Validation_HD' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(X_Validation_HD[0]))\n",
    "\n",
    "# print(X_train_HD[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"File.txt\", \"a\")\n",
    "# f.write(Training_set)\n",
    "# f.write(\"\\n\")\n",
    "# f.write(Validation_set)\n",
    "# f.write(\"\\n\")\n",
    "# f.write(Test_set)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDistance(Dt,D1):\n",
    "    count = 0\n",
    "    for i in range(len(D1)):\n",
    "        if D1[i] != Dt[i]:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HammingDistance(Dt, X_train, Y_train):\n",
    "    Hamming_D = []\n",
    "    for i in range(len(X_train)):\n",
    "#         Hamming_D.append(len(Dt)*distance.hamming(Dt, X_train[i]))\n",
    "        Hamming_D.append(getDistance(Dt, X_train[i]))\n",
    "    \n",
    "    output = []\n",
    "#     print(Hamming_D)\n",
    "    for k in K:\n",
    "        indices = sorted(range(len(Hamming_D)), key = lambda sub: Hamming_D[sub])[:k] \n",
    "        Y_output = []\n",
    "        for i in indices:\n",
    "            Y_output.append(Y_train[i])   \n",
    "    \n",
    "        output.append(max(set(Y_output), key = Y_output.count))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def HammingDistance_Accuracy(X_Validation_HD, Y_Validation, X_train_HD, Y_train):\n",
    "    count = [0,0,0]\n",
    "    for i in range(len(X_Validation_HD)):\n",
    "        result = HammingDistance(X_Validation_HD[i], X_train_HD, Y_train)\n",
    "        \n",
    "        for k in range(len(K)):\n",
    "            if Y_Validation[i] == result[k] :\n",
    "                count[k] += 1\n",
    "        \n",
    "        print(i, count)\n",
    "        \n",
    "    for k in range(len(K)):\n",
    "        print(\"Accuracy is: \", (count[k]/len(Y_Validation) * 100))\n",
    "\n",
    "# print(HammingDistance(X_test_HD[1200], X_train_HD, Y_train))\n",
    "# print(Y_test[1200])\n",
    "# HammingDistance_Accuracy(X_Validation_HD, Y_Validation, X_train_HD, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateED(Dt,D):\n",
    "    value = 0\n",
    "    for i in range(len(D)):\n",
    "        value += pow(Dt[i] - D[i], 2)\n",
    "    value = math.sqrt(value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EuclideanDistance(Dt, X_train, Y_train):\n",
    "    Euclidean_D = []\n",
    "    for i in range(len(X_train)):\n",
    "#         Euclidean_D.append(CalculateED(Dt, X_train[i]))\n",
    "        Euclidean_D.append(distance.euclidean(Dt, X_train[i]))\n",
    "        \n",
    "    \n",
    "    output = []\n",
    "    for k in K:\n",
    "    #find k minimum indices\n",
    "        indices = sorted(range(len(Euclidean_D)), key = lambda sub: Euclidean_D[sub])[:k] \n",
    "        Y_output = []\n",
    "        for i in indices:\n",
    "            Y_output.append(Y_train[i])   \n",
    "    \n",
    "        #find higher frequency result\n",
    "        output.append(max(set(Y_output), key = Y_output.count)) \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EuclideanDistance_Accuracy(X_Validation_ED, Y_Validation, X_train_ED, Y_train):\n",
    "    count = [0, 0, 0]\n",
    "    for i in range(len(X_Validation_ED)):\n",
    "        result = EuclideanDistance(X_Validation_ED[i], X_train_ED, Y_train)\n",
    "        \n",
    "        for k in range(len(K)):\n",
    "            if Y_Validation[i] == result[k] :\n",
    "                count[k] += 1\n",
    "        \n",
    "        print(i,count)\n",
    "    \n",
    "    for k in range(len(K)):\n",
    "        print(\"Accuracy is: \", (count[k]/len(Y_Validation) * 100))\n",
    "\n",
    "# HammingDistance(X_Validation_HD[0], X_train_HD, Y_train)\n",
    "# print(Y_Validation[0])\n",
    "\n",
    "# EuclideanDistance_Accuracy(X_Validation_ED, Y_Validation, X_train_ED, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wood_Working', 'Anime', 'Coffee']\n",
      "Coffee\n",
      "29.052965879440308\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(EuclideanDistance(X_Validation_HD[0], X_train_HD, Y_train))\n",
    "print(Y_Validation[0])\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_documents = len(X_train_HD)\n",
    "d_word = np.sum(X_train_HD.copy(), axis=0)\n",
    "# print(d_word)\n",
    "# print(len(d_word))\n",
    "IDF = []\n",
    "for i in range(len(d_word)):\n",
    "    temp = math.log2(number_of_documents/d_word[i])\n",
    "    if temp <= 0:\n",
    "        IDF.append(0.0001)\n",
    "    else:\n",
    "        IDF.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setWeight(D, Dt):\n",
    "    totalWords_D = sum(D)\n",
    "    totalWords_Dt = sum(Dt)\n",
    "    for i in range(len(D)):\n",
    "        if D[i] != 0 :\n",
    "            D[i] = (D[i]/totalWords_D) * IDF[i]\n",
    "        if Dt[i] != 0:\n",
    "            Dt[i] = (Dt[i]/totalWords_Dt) * IDF[i]\n",
    "    return D,Dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setWeightOfDocument(D):\n",
    "    totalWords_D = sum(D)\n",
    "    for i in range(len(D)):\n",
    "        if D[i] != 0 :\n",
    "            D[i] = (D[i]/totalWords_D) * IDF[i]\n",
    "    return D\n",
    "\n",
    "def SetAllWeightOfSET(X_train):\n",
    "#     count = 0\n",
    "    for D in X_train:\n",
    "        D = setWeightOfDocument(D)\n",
    "#         count += 1\n",
    "#         print(count)\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = SetAllWeightOfSET(deepcopy(X_train_ED))\n",
    "# V = SetAllWeightOfSET(deepcopy(X_Validation_ED))\n",
    "# print(X_train_ED[0])\n",
    "# print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CosineTheta(D , Dt):\n",
    "    return np.dot(D,Dt)/(norm(D)*norm(Dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CosineSimilarity(Dt, X_train, Y_train):\n",
    "    Cosine_values = []\n",
    "#     output = []\n",
    "    for D in X_train:\n",
    "        Cosine_values.append(1-distance.cosine(Dt,D))\n",
    "\n",
    "#     for k in K:\n",
    "    indices = (sorted(range(len(Cosine_values)), key = lambda sub: Cosine_values[sub])[-5:])\n",
    "    Y_output = []\n",
    "    for i in indices:\n",
    "        Y_output.append(Y_train[i])\n",
    "\n",
    "    output= (max(set(Y_output), key = Y_output.count)) \n",
    "#     print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def CosineValidation_Test(X_Validation_ED, Y_Validation, X_train_ED, Y_train):\n",
    "#     count = [0,0,0]\n",
    "    count = 0\n",
    "    for i in range(len(X_Validation_ED)):\n",
    "#         result = CosineSimilarity(X_Validation_ED[i], X_train_ED, Y_train)\n",
    "        \n",
    "#         for k in range(len(K)):\n",
    "        if Y_Validation[i] ==  CosineSimilarity(X_Validation_ED[i], X_train_ED, Y_train):\n",
    "            count+= 1\n",
    "        \n",
    "        print(i, count)\n",
    "    \n",
    "#     for k in range(len(K)):\n",
    "#         print(\"Accuracy is: \", (count[k]/len(Y_Validation) * 100))\n",
    "    \n",
    "    return (count/len(Y_Validation) * 100)\n",
    "\n",
    "# CosineValidation_Test(V, deepcopy(Y_Validation), X, deepcopy(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coffee\n",
      "85.88710808753967\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "CosineSimilarity(deepcopy(X_Validation_ED[0]),deepcopy(X_train_ED), deepcopy(Y_train))\n",
    "print(Y_Validation[0])\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denominatorOfNB(Dt,alpha):\n",
    "    summation = 0\n",
    "    \n",
    "    for i in range(len(TopicName)):\n",
    "        summation += Prob_DT_CM(Dt,i, alpha)\n",
    "    \n",
    "    return summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prob_Wj_Cm(word, m, alpha):\n",
    "    \n",
    "    frequencyOf_W_in_M = 0\n",
    "    TotalWords_in_M = len(CM_list[m])\n",
    "    \n",
    "    for i in range(len(CM_list[m])):\n",
    "        frequencyOf_W_in_M += CM_list[m][i].count(word)\n",
    "        \n",
    "    value = (frequencyOf_W_in_M + alpha)/ (TotalWords_in_M + alpha * len(FeatureSpace))\n",
    "    \n",
    "    if value == 0:\n",
    "        print(m, \"e jhamela ache \", value, \"->\", word)\n",
    "\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prob_DT_CM(Dt, m, alpha):\n",
    "    P_Dt_Cm = 1\n",
    "    \n",
    "    for i in range(len(Dt)):\n",
    "        if(Dt[i] != 0):\n",
    "            P_Dt_Cm *= Prob_Wj_Cm(FeatureSpace[i],m, alpha)\n",
    "#             print(P_Dt_Cm)\n",
    "    \n",
    "#     print(\"probab of DT being in \", m, \" is \", P_Dt_Cm)\n",
    "    return P_Dt_Cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaiveBayes(Dt, m, denom, alpha):\n",
    "    value = (Prob_DT_CM(Dt,m,alpha) * (1/len(CM_list)))/ (denom + alpha*len(FeatureSpace))\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1.5, 0.2]\n"
     ]
    }
   ],
   "source": [
    "# Probability = []\n",
    "# denom = denominatorOfNB(X_Validation_ED[6])\n",
    "# # for i in range(len(CM_list)):\n",
    "# #     Probability.append(NaiveBayes(X_Validation_ED[6],i, denom))\n",
    "\n",
    "# # # print(Probability)    \n",
    "# # print(TopicName[Probability.index(max(Probability))][0])\n",
    "# print(denom)\n",
    "\n",
    "# print(len(X_Validation_HD[6]))\n",
    "# # len(CM_list)\n",
    "\n",
    "alpha = [2, 1, 1.5, 0.5, 0.2, 0.1, 0.07, 0.05, 0.01, 0.005] \n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB_precision(X_Validation_HD, Y_Validation, alpha):\n",
    "    count = 0\n",
    "    for itr in range(len(X_Validation_HD)):\n",
    "        Probability = []\n",
    "        denom = denominatorOfNB(X_Validation_HD[itr], alpha)\n",
    "#         if denom == 0:\n",
    "#             continue\n",
    "        for i in range(len(CM_list)):\n",
    "            Probability.append(NaiveBayes(X_Validation_HD[itr],i, denom, alpha))\n",
    "        \n",
    "        prediction = TopicName[Probability.index(max(Probability))][0]\n",
    "        if prediction == Y_Validation[itr]:\n",
    "            count += 1\n",
    "#             print(count)\n",
    "    \n",
    "#     print(\"Accuracy is: \", (count/len(Y_Validation) * 100)) \n",
    "    return ((count/len(Y_Validation)) * 100)\n",
    "\n",
    "# NB_precision(X_Validation_HD, Y_Validation, 0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for alpha in x:\n",
    "#     NB_precision(X_Validation_HD, Y_Validation, alpha)\n",
    "#     print(alpha)\n",
    "#     print(\"--------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicName = []\n",
    "x=[d.keys() for d in Training_set]\n",
    "for keys in x:\n",
    "    if list(keys) not in TopicName:\n",
    "        TopicName.append(list(keys))\n",
    "\n",
    "CM_list = []\n",
    "\n",
    "for j in range(len(TopicName)):\n",
    "    CM_list.append([])\n",
    "    for i in range(len(Training_set)):\n",
    "        if list(Training_set[i].keys()) == TopicName[j]:\n",
    "            y = list(Training_set[i].values())[0]\n",
    "            CM_list[j].append(y)\n",
    "            \n",
    "# len(CM_list[1])\n",
    "# len(FeatureSpace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestValue(itr):\n",
    "#     print(len(Test_set))\n",
    "    X_test_HD = []\n",
    "    X_test_ED = []\n",
    "    Y_test = []\n",
    "\n",
    "    # print(inputs)\n",
    "    Test_Final = []\n",
    "    for input_ in inputs:\n",
    "#         print(input_)\n",
    "        temp = [Test for Test in Test_set if list(Test.keys())[0] == input_ ]\n",
    "        Test_Final += temp[itr*10:itr*10+10]\n",
    "\n",
    "    for i in range(len(Test_Final)):\n",
    "        Y_test.append(list(Test_Final[i].keys())[0])\n",
    "        temp = list(Test_Final[i].values())[0]\n",
    "        HD = []\n",
    "        ED = []\n",
    "        \n",
    "        for i in range(len(FeatureSpace)):\n",
    "            if FeatureSpace[i] in temp:\n",
    "                HD.append(1)\n",
    "            else:\n",
    "                HD.append(0)\n",
    "                \n",
    "            ED.append(temp.count(FeatureSpace[i]))\n",
    "                \n",
    "        X_test_HD.append(HD)\n",
    "        X_test_ED.append(ED)\n",
    "        \n",
    "    return X_test_HD, Y_test, X_test_ED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(50):\n",
    "    X_test_HD, Y_test, X_train_ED = getTestValue()\n",
    "    \n",
    "    with open('NaiveBayes.txt', 'a') as f:\n",
    "        f.write(\"%s\\n\" %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestData extracted\n",
      "Done iteration NB 0 86.36363636363636\n",
      "X_Test weight set\n",
      "0 0\n",
      "1 1\n",
      "2 2\n",
      "3 3\n",
      "4 4\n",
      "5 5\n",
      "6 5\n",
      "7 6\n",
      "8 7\n",
      "9 8\n",
      "10 9\n",
      "11 10\n",
      "12 11\n",
      "13 12\n",
      "14 13\n",
      "15 14\n",
      "16 15\n",
      "17 16\n",
      "18 17\n",
      "19 18\n",
      "20 19\n",
      "21 20\n",
      "22 21\n",
      "23 22\n",
      "24 22\n",
      "25 23\n",
      "26 23\n",
      "27 24\n",
      "28 25\n",
      "29 26\n",
      "30 27\n",
      "31 28\n",
      "32 29\n",
      "33 30\n",
      "34 31\n",
      "35 32\n",
      "36 33\n",
      "37 34\n",
      "38 35\n",
      "39 36\n",
      "40 36\n",
      "41 37\n",
      "42 38\n",
      "43 38\n",
      "44 39\n",
      "45 40\n",
      "46 40\n",
      "47 41\n",
      "48 42\n",
      "49 43\n",
      "50 44\n",
      "51 45\n",
      "52 46\n",
      "53 47\n",
      "54 48\n",
      "55 49\n",
      "56 50\n",
      "57 51\n",
      "58 52\n",
      "59 53\n",
      "60 54\n",
      "61 55\n",
      "62 56\n",
      "63 56\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(50):\n",
    "    X_test_HD, Y_test, X_test_ED = getTestValue(iteration)\n",
    "    print(\"TestData extracted\")\n",
    "    \n",
    "    accuracy_NB = NB_precision(X_test_HD, Y_test, 0.005)\n",
    "    print(\"Done iteration NB\", iteration, accuracy_NB)\n",
    "    with open('NaiveBayes.txt', 'a') as f:\n",
    "        f.write(\"%s\\n\" %accuracy_NB)\n",
    "    \n",
    "    X_TEST = SetAllWeightOfSET(deepcopy(X_test_ED))\n",
    "    print(\"X_Test weight set\")\n",
    "    accuracy_CS = CosineValidation_Test(X_TEST, deepcopy(Y_test), X, deepcopy(Y_train))\n",
    "    print(\"Done iteration CS\", iteration, accuracy_CS)    \n",
    "    with open('CosineSim.txt', 'a') as f2:\n",
    "        f2.write(\"%s\\n\" %accuracy_CS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
